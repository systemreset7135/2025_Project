import cv2
import numpy as np
import tflite_runtime.interpreter as tflite
from tflite_runtime.interpreter import load_delegate

# 모델과 라벨 파일을 한 번만 로드합니다.
MODEL_PATH = "/home/limelight/model.tflite"   # 실제 모델 파일 경로로 수정하세요.
LABELS_PATH = "/home/limelight/labels.txt"      # 실제 라벨 파일 경로로 수정하세요.

# Coral EdgeTPU용 TFLite 인터프리터 생성 (USB 3.0 Coral이 내장되어 있다고 가정)
interpreter = tflite.Interpreter(
    model_path=MODEL_PATH,
    experimental_delegates=[load_delegate('libedgetpu.so.1')]
)
interpreter.allocate_tensors()

# 모델의 입력/출력 세부 정보를 가져옵니다.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
input_shape = input_details[0]['shape']  # 예: [1, height, width, 3]
input_size = input_shape[1:3]            # [height, width]

# 라벨 파일 로드 (모델에 따른 클래스 라벨)
with open(LABELS_PATH, 'r') as f:
    labels = [line.strip() for line in f.readlines()]

def preprocess_image(image):
    """
    이미지를 모델의 입력 크기에 맞게 리사이즈하고 RGB로 변환한 후,
    배치 차원을 추가하여 반환합니다.
    """
    resized = cv2.resize(image, (input_size[1], input_size[0]))
    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
    # 여기서는 모델이 uint8 입력을 사용한다고 가정합니다.
    return np.expand_dims(rgb, axis=0).astype(np.uint8)

def runPipeline(image, llrobot):
    """
    Limelight의 백엔드가 매 프레임 호출하는 함수입니다.
    
    매 프레임마다:
      1. 전달된 이미지를 전처리합니다.
      2. Coral TPU를 이용해 TFLite 모델 추론을 수행합니다.
      3. 추론 결과(바운딩 박스, 클래스, 신뢰도)를 파싱하여, 가장 신뢰도 높은 검출 결과를 찾습니다.
      4. 검출된 객체의 바운딩 박스 중심 좌표를 계산하고, 이를 이미지 중앙 기준(-0.5 ~ +0.5) 오프셋으로 변환합니다.
      5. 시각화를 위해 원본 이미지에 바운딩 박스와 라벨을 그립니다.
      6. 최종적으로 Limelight가 요구하는 튜플 (largestContour, image, llpython)을 반환합니다.
         - 여기서 largestContour는 Limelight의 교차선 기능에 사용되며, 필요 시 여러분이 직접 컨투어를 반환할 수 있습니다.
         - llpython 배열에는 [offsetX, offsetY, confidence, class_id, 0, 0, 0, 0]와 같이 커스텀 데이터를 담습니다.
    """
    # 1. 이미지 전처리
    input_data = preprocess_image(image)

    # 2. 추론 수행
    interpreter.set_tensor(input_details[0]['index'], input_data)
    interpreter.invoke()

    # 3. 출력 텐서 가져오기 (SSD 모델 기준: detection_boxes, detection_classes, detection_scores)
    boxes = interpreter.get_tensor(output_details[0]['index'])[0]      # shape: [num, 4]
    classes = interpreter.get_tensor(output_details[1]['index'])[0]    # shape: [num]
    scores = interpreter.get_tensor(output_details[2]['index'])[0]     # shape: [num]

    # 4. 가장 높은 신뢰도의 검출 결과 선택 (신뢰도 0.5 이상인 경우)
    best_detection = None
    best_score = 0.0
    for i in range(len(scores)):
        if scores[i] > best_score and scores[i] > 0.5:
            best_score = scores[i]
            best_detection = (boxes[i], classes[i], scores[i])

    if best_detection is not None:
        box, class_id, score = best_detection
        ymin, xmin, ymax, xmax = box
        # 바운딩 박스 중심 좌표 계산 (정규화된 값: 0~1)
        center_x = (xmin + xmax) / 2.0
        center_y = (ymin + ymax) / 2.0

        # 5. 이미지에 바운딩 박스와 라벨 그리기
        h, w = image.shape[:2]
        pt1 = (int(xmin * w), int(ymin * h))
        pt2 = (int(xmax * w), int(ymax * h))
        cv2.rectangle(image, pt1, pt2, (0, 255, 0), 2)
        label = f"{labels[int(class_id)]}: {score:.2f}"
        cv2.putText(image, label, (int(xmin * w), int(ymin * h) - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

        # 6. 이미지 중심 기준 오프셋 계산 (-0.5 ~ +0.5)
        offsetX = center_x - 0.5
        offsetY = center_y - 0.5

        # 동적 회피 방향 결정에 활용할 수 있는 데이터 배열을 생성합니다.
        llpython = [offsetX, offsetY, score, class_id, 0, 0, 0, 0]
    else:
        # 검출 결과가 없으면 기본값 반환
        llpython = [0, 0, 0, -1, 0, 0, 0, 0]

    # Limelight의 교차선 기능에 사용할 컨투어를 반환할 수 있으나, 여기서는 사용하지 않으므로 빈 배열 반환
    largestContour = np.array([[]])
    return largestContour, image, llpython
